{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Linear Algebra\n",
    "\n",
    "Now that you can store and manipulate data, let's briefly review the subset of basic linear algebra that you'll need to understand most of the models. We'll introduce all the basic concepts, the corresponding mathematical notation, and their realization in code all in one place. If you're already confident basic linear algebra, free to skim or skip this chapter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import nd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalars\n",
    "\n",
    "If you never studied linear algebra or machine learning, you're probably used to working with single numbers, like $42.0$ and know how to do basic things like add them together, multiply them. In mathematical notation, we'll represent salars with ordinary lower cased letters ($x$, $y$, $z$). In MXNet, we can work with scalars by creating NDArrays with just one element. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = nd.array([3.0]) \n",
    "y = nd.array([2.0])\n",
    "print('x + y = ', x + y)\n",
    "print('x * y = ', x * y)\n",
    "print('x / y = ', x / y)\n",
    "print('x ** y = ', nd.power(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert NDArrays to Python floats by calling their ``.asscalar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.asscalar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors \n",
    "You can think of vectors are simply a list of numbers ([1.0,3.0,4.0,2.0]). A vector could represent numerical features of some real-world person or object, like the last-record measurements across various vital signs for a patient in the hospital. In math notation, we'll always denote vectors as bold-faced lower-cased letters ($\\boldsymbol{u}$, $\\boldsymbol{v}$, $\\boldsymbol{w})$. In MXNet, we work with vectors via 1D NDArrays with an arbitrary number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = nd.zeros(4)\n",
    "print('u = ', u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can refer to any element of a vector by using a subscript. For example, we can refer to the $4$th element of $\\boldsymbol{u}$ by $u_4$. Note that the element $u_4$ is a scalar, so we don't bold-face the font when referring to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices\n",
    "\n",
    "Just as vectors are an extension of scalars from 0 to 1 dimension, matrices generalization vectors to two dimensions. Matrices, which we'll denote with capital letters ($A$, $B$, $C$) are 2D arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = nd.arange(20).reshape((5,4))\n",
    "print('A = ', A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrices are useful data structures, they allow us to organize data that has different modalities of variation. For example, returning to the example of medical data, rows in our matrix might correspond to different patients, while columns might correspond to different attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the scalar elements $a_{ij}$ of a matrix A by specifying the indices for the row ($i$) and column ($j$) respectively. Let's grab the element $a_{2,3}$ from the random matrix we initialized above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('A[2,3] = ', A[2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also grab the vectors corresponding to entire rows $\\boldsymbol{a}_{i,:}$ or columns $\\boldsymbol{a}_{:,j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('row 2', A[2,:])\n",
    "print('column 3', A[:,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can transpose the matrix through `T`. That is, if $B = A^T$, then $B_{i,j} = A_{j,i}$ for any $i$ ane $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors \n",
    "\n",
    "Just as vectors generalize scalars, and matrices generalize vectors, we can actually build data structures with even more axes. Tensors, give us a generic way of discussing arrays with an arbitrary number of axes. Vectors, for example are first-order tensors, and matrices are second-order tensors.\n",
    "\n",
    "Using tensors will become more important when we start working with images, which arrive as 3D data structures, with axes corresponding to the height, width, and the three (RGB) color channels. But in this chapter, we're going to skip past and make sure you know the basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = nd.arange(24).reshape((2,3,4))\n",
    "print('X.shape =', X.shape)\n",
    "print('X =', X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Element-wise operations\n",
    "\n",
    "Oftentimes, we want to perform element-wise operations. This means that we perform a scalar operation on the corresponding elements of two vectors. So given any two vectors $\\boldsymbol{u}$ and $\\boldsymbol{v}$ *of the same shape*, and a scalar function $f$, we can perform the operation  we produce vector $\\boldsymbol{c} = f(\\boldsymbol{u},\\boldsymbol{v})$ by setting $c_i \\gets f(u_i, v_i)$. In MXNet, calling any of the standard arithmetic operators (+,-,/,\\*,\\*\\*) will invoke an elementwise operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = nd.ones_like(u) * 2\n",
    "print('v =', v)\n",
    "print('u + v', u + v)\n",
    "print('u - v', u - v)\n",
    "print('u * v', u * v)\n",
    "print('u / v', u / v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call element-wise operations on any two tensors of the same shape, including matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = nd.ones_like(A) * 3\n",
    "print('B =', B)\n",
    "print('A + B =', A + B)\n",
    "print('A * B =', A * B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sums and means \n",
    "\n",
    "The next more sophisticated thing we can do with arbitrary tensors is to calculate the sum of their elements. In mathematical notation, we express sums using the $\\sum$ symbol. To express the sum of the elements in a vector $\\boldsymbol{u}$ of length $d$, we can write $\\sum_{i=1}^d u_i$. In code, we can just call ``nd.sum()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd.sum(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can similarly express sums over the elements of tensors of arbitrary shape. For example, the sum of the elements of an $m \\times n$ matrix A could be written $\\sum_{i=1}^{m} \\sum{j=1}^{n} a_{i,j}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd.sum(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A related quantity to the sum is the *mean*, also commonly called the *average*. We calculate the mean by dividing the sum by the total number of elements. With mathematical notation, we could write the average over a vector ${\\boldsymbol{u}$ as \\frac{1}{d} \\sum_{i=1}^{d} u_i$ and the average over a matrix $A$ as  $\\frac{1}{n \\cdot m} \\sum_{i=1}^{m} \\sum_{j=1}^{n} a_{i,j}$. In code, we could just call ``nd.mean()`` tensors of arbitrary shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd.mean(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot products\n",
    "\n",
    "<!-- So far, we've only performed element-wise operations, sums and averages. And if this was we could do, linear algebra probably wouldn't deserve it's own chapter. However, -->\n",
    "\n",
    "One of the most fundamental operations is the dot product. Given two vectors $\\boldsymbol{u}$ and $\\boldsymbol{v}$, the dot product $\\boldsymbol{u}^T \\cdot \\boldsymbol{v}$ is a sum over the products of the corresponding elements: $\\boldsymbol{u}^T \\cdot \\boldsymbol{v} = \\sum_{i=1}^{d} u_i \\cdot v_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd.dot(u,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can code the dot product over two vectors ``nd.dot(u, v)`` equivalently by performing an element-wise multiplication and then a sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd.sum(u * v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dot products are useful in a wide range of contexts. For example, given a set of weights $\\boldsymbol{w}$, the weighted sum of some values ${u}$ could be expressed as the dot product $\\boldsymbol{u}^T \\boldsymbol{w}$. When the weights are non-negative and sum to one ($\\sum_{i=1}^{d} {w_i} = 1$), the dot product expresses a *weighted average*. When two vectors each have length one (we'll discuss what *length* means below in the section on norms), dot products can also capture the cosine of the angle between two vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix-vector products\n",
    "\n",
    "Now that we know how to calculate dot products we can begin to understand matrix-vector products. Let's start off by visualizing a matrix $A$ and a column vector $\\boldsymbol{x}$.\n",
    "\n",
    "$$\\mathbf{A}=\\begin{pmatrix}\n",
    " a_{11} & a_{12} & \\cdots & a_{1m} \\\\\n",
    " A_{21} & a_{22} & \\cdots & a_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " a_{n1} & a_{n2} & \\cdots & a_{nm} \\\\\n",
    "\\end{pmatrix},\\quad\\mathbf{x}=\\begin{pmatrix}\n",
    " \\mathbf{x}_{1}  \\\\\n",
    " \\mathbf{x}_{2} \\\\\n",
    "\\vdots\\\\\n",
    " \\mathbf{x}_{m}\\\\\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "We can visualize the matrix in terms of its row vectors\n",
    "\n",
    "$$\\mathbf{A}=\n",
    "\\begin{pmatrix}\n",
    "\\cdots & \\mathbf{a}^T_{1} &...  \\\\\n",
    "\\cdots & \\mathbf{a}^T_{2} & \\cdots \\\\\n",
    " & \\vdots &  \\\\\n",
    " \\cdots &\\mathbf{a}^T_n & \\cdots \\\\\n",
    "\\end{pmatrix},$$\n",
    "\n",
    "where each $\\mathbf{a}^T_{i} \\in \\mathcal{R}^{m}$\n",
    "is a row vector representing the $i$-th row of the matrix A.\n",
    "\n",
    "Then the matrix vector product $\\mathbf{y} = A\\mathbf{x}$ is simply a column vector $y \\in \\mathcal{R^n}$ where each entry $y_i$ is the dot product $\\mathbf{a}^T_i \\cdot \\mathbf{x}$.\n",
    "\n",
    "$$\\mathbf{A}=\n",
    "\\begin{pmatrix}\n",
    "\\cdots & \\mathbf{a}^T_{1} &...  \\\\\n",
    "\\cdots & \\mathbf{a}^T_{2} & \\cdots \\\\\n",
    " & \\vdots &  \\\\\n",
    " \\cdots &\\mathbf{a}^T_n & \\cdots \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    " x_{1}  \\\\\n",
    " x_{2} \\\\\n",
    "\\vdots\\\\\n",
    " x_{m}\\\\\n",
    "\\end{pmatrix}\n",
    "= \\begin{pmatrix}\n",
    " \\mathbf{a}^T_{1} \\cdot \\mathbf{x}  \\\\\n",
    " \\mathbf{a}^T_{2} \\cdot \\mathbf{x} \\\\\n",
    "\\vdots\\\\\n",
    " \\mathbf{a}^T_{n} \\mathbf{x}\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "So you can think of multiplication by a matrix $A\\in \\mathcal{R}^{n \\times n}$ as a transformation that projects vectors from $\\mathcal{R}^{m}$ to $\\mathcal{R}^{n}$.\n",
    "\n",
    "These transformations turn out to be quite useful. For example, we can represent rotations as multiplications by a square matrix. As we'll see in subsequent chapters, we can also use matrix-vector products to describe the calculations of each layer in a neural network. \n",
    "\n",
    "Expressing matrix-vector products in code with ``ndarray``, we use the same ``nd.dot()`` function as for dot products. When we call ``nd.dot(A, x)`` with a matrix ``A`` and a vector ``x``, ``mxnet`` knows to perform a matrix-vector product. Note that the column dimension of ``A`` must be the same as the dimension of ``x``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd.dot(A, u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix-matrix multiplication\n",
    "\n",
    "If you've gotten the hang of dot products and matrix-vector multiplication, then matrix-matrix multiplications should be pretty straightforward.\n",
    "\n",
    "Say we have two matrices, $A \\in \\mathcal{R}^{n \\times k}$ and $B \\in \\mathcal{R}^{k \\times m}$:\n",
    "\n",
    "$$\\mathbf{A}=\\begin{pmatrix}\n",
    " a_{11} & a_{12} & \\cdots & a_{1k} \\\\\n",
    " a_{21} & a_{22} & \\cdots & a_{2k} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " a_{n1} & a_{n2} & \\cdots & a_{nk} \\\\\n",
    "\\end{pmatrix},\\quad\n",
    "\\mathbf{B}=\\begin{pmatrix}\n",
    " b_{11} & b_{12} & \\cdots & b_{1m} \\\\\n",
    " b_{21} & b_{22} & \\cdots & b_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " b_{k1} & b_{k2} & \\cdots & b_{km} \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "To produce the matrix product $C = AB$, it's easiest to think of $A$ in terms of its row vectors and $B$ in terms of its column vectors:\n",
    "\n",
    "$$\\mathbf{A}=\n",
    "\\begin{pmatrix}\n",
    "\\cdots & \\mathbf{a}^T_{1} &...  \\\\\n",
    "\\cdots & \\mathbf{a}^T_{2} & \\cdots \\\\\n",
    " & \\vdots &  \\\\\n",
    " \\cdots &\\mathbf{a}^T_n & \\cdots \\\\\n",
    "\\end{pmatrix},\n",
    "\\quad \\mathbf{B}=\\begin{pmatrix}\n",
    "\\vdots & \\vdots &  & \\vdots \\\\\n",
    " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
    " \\vdots & \\vdots &  &\\vdots\\\\\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "Note here that each row vector $\\mathbf{a}^T_{i}$ lies in $\\mathcal{R}^k$ and that each column vector $\\mathbf{b}$ also lies in $\\mathcal{R}^k$.\n",
    "\n",
    "Then to produce the matrix product $C \\in \\mathcal{R}^{n \\times m}$ we simply compute each entry $c_{ij}$ as the dot product $\\mathbf{a}^T_i \\cdot \\mathbf{b}_j$.\n",
    "\n",
    "$$\\begin{pmatrix}\n",
    "\\cdots & \\mathbf{a}^T_{1} &...  \\\\\n",
    "\\cdots & \\mathbf{a}^T_{2} & \\cdots \\\\\n",
    " & \\vdots &  \\\\\n",
    " \\cdots &\\mathbf{a}^T_n & \\cdots \\\\\n",
    "\\end{pmatrix} \\cdot\n",
    "\\begin{pmatrix}\n",
    "\\vdots & \\vdots &  & \\vdots \\\\\n",
    " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
    " \\vdots & \\vdots &  &\\vdots\\\\\n",
    "\\end{pmatrix}\n",
    "= \\begin{pmatrix}\n",
    "\\mathbf{a}^T_{1} \\cdot \\mathbf{b}_1 & \\mathbf{a}^T_{1} \\cdot \\mathbf{b}_2&  & \\mathbf{a}^T_{1} \\cdot \\mathbf{b}_m \\\\\n",
    " \\mathbf{a}^T_{2} \\cdot \\mathbf{b}_1 & \\mathbf{a}^T_{2} \\cdot \\mathbf{b}_2 & \\cdots & \\mathbf{a}^T_{2} \\cdot \\mathbf{b}_m \\\\\n",
    " \\vdots & \\vdots &  &\\vdots\\\\\n",
    "\\mathbf{a}^T_{n} \\cdot \\mathbf{b}_1 & \\mathbf{a}^T_{n} \\cdot \\mathbf{b}_2& \\cdots& \\mathbf{a}^T_{n} \\cdot \\mathbf{b}_m \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "You can think of the matrix-matrix multiplication $AB$ as simply performing $m$ matrix-vector products and stitching the results together to form an $n \\times m$ matrix.\n",
    "\n",
    "Just as with ordinary dot products and matrix-vector products, we can computer matrix-matrix products in ``mxnet`` by using ``nd.dot()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd.dot(A.T, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Before we can start implementing models, \n",
    "there's one last concept we're going to introduce. \n",
    "Some of the most useful operators in linear algebra are norms.\n",
    "Informally, they tell ushow big a vector or matrix is. \n",
    "We represent norms with the notation $||\\cdot||$. \n",
    "The $\\cdot$ in this expression is just a placeholder. \n",
    "For example, we would represent the norm of a vector $\\mathbf{x}$ \n",
    "or matrix $A$ as $||\\mathbf{x}||$ or $||A||$, respectively. \n",
    "\n",
    "All norms must satisfy a handful of properties:\n",
    "1. $||\\alpha A|| = |\\alpha| ||A||$\n",
    "2. $||A + B|| \\leq ||A|| + ||B||$\n",
    "3. $||A|| \\geq 0$\n",
    "4. If $\\forall {i,j}, a_{ij} = 0$, then $||A||=0$\n",
    "\n",
    "To put it in words, the first rule says \n",
    "that if we scale all the components of a matrix or vector \n",
    "by a constant factor $\\alpha$, \n",
    "its norm also scales by the *absolute value* \n",
    "of the same constant factor. \n",
    "The second rule is the familiar triangle inequality.\n",
    "The third rule simple says that the norm must be non-negative. \n",
    "That makes sense, in most contets the smallest *size* for anything is 0.\n",
    "The final rule basically says that the smallest norm is achieved by a matrix or vector consisting of all zeros.\n",
    "It's possible to define a norm that gives zero norm to nonzero matrices,\n",
    "but you can't give nonzero norm to zero matrices. \n",
    "That's a mouthful, but if you digest it then you probably have grepped the important concepts here.\n",
    "\n",
    "If you remember Euclidean distances (think Pythagoras' theorem) from gradeschool, \n",
    "then non-negativity and the triangle inequality might ring a bell.\n",
    "You might notice that norms sound a lot like measures of distance.\n",
    "\n",
    "In fact the Euclidean distance $\\sqrt{x_1^2 + \\cdots + x_n^2}$ is a norm. \n",
    "Specifically it's the $\\ell_2$-norm. \n",
    "When applied over the entries of a matrix, e.g. $\\sqrt{\\sum_{i,j} a_{ij}}$, \n",
    "the $\\ell_2$ norm is also called the Frobenius norm. \n",
    "More often, in machine learning we work with with the squared $\\ell_2$ norm (notated $\\ell_2^2$).\n",
    "We also commonly work with the $\\ell_1$ norm.\n",
    "The $\\ell_1$ norm is simply the sum of the absolute values. \n",
    "It has the convenient property of placing less emphasis on outliers.\n",
    "\n",
    "To calculate the $\\ell^2_2$ norm, we can just call ``nd.norm()``.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd.norm(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the one norm we can simply perform the absolute value and then sum over the elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd.sqrt(nd.sum(nd.abs(u)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norms and Objectives\n",
    "\n",
    "While we don't want to get to far ahead of ourselves, we do want you to anticipate why these concepts are useful.\n",
    "In machine learning we're often trying to solve optimization problems: *Maximize* the probability assigned to observed data. *Minimize* the distance between predictions and the groundtruth observations. Assign vector represenatations to items (like words, products, or news articles) such that the distance between similar items is minimized, and the distance between dissimilar times is maximized. Oftentimes, these objectives, perhaps the most important component of a machine learning algorithm (besides the data itself), are expressed as norms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "In just a few pages (or one Jupyter notebook) we've taught you all the linear algebra you'll need to understand a good chunk of neural networks. Of course there's a *lot* more to linear algebra. And a lot of that math *is* useful for machine learning. For example, matrices can be decomposed into factors, and these decompositions can reveal low-dimensinoal structure in real-world datasets. There are entire subfields of machine learning that focus on using matrix decompositions and their generalizations to high-order tesors to discover structure in datasets and solve prediction problems. But this book focuses on deep learning. And we believe you'll be much more inclined to learn more mathematics once you've gotten your hands dirty deploying useful machine learning models on real datasets. So while reserve the right to introduce more math much later on, we'll wrap up this chapter here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For whinges or inquiries, [open an issue on  GitHub.](https://github.com/zackchase/mxnet-the-straight-dope)"
   ]
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
